** ComfyUI startup time: 2024-03-25 20:24:11.589318
** Platform: Windows
** Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
** Python executable: C:\Users\silvr\anaconda3\envs\Diffuser\python.exe
** Log path: C:\Users\silvr\OneDrive\Desktop\ComfyUI\comfyui.log

Prestartup times for custom nodes:
   0.0 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\ComfyUI-Manager

Total VRAM 16376 MB, total RAM 32602 MB
Set vram state to: NORMAL_VRAM
Device: cuda:0 NVIDIA GeForce RTX 4080 : cudaMallocAsync
VAE dtype: torch.bfloat16
Using pytorch cross attention
C:\Users\silvr\anaconda3\envs\Diffuser\Lib\site-packages\transformers\utils\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
### Loading: ComfyUI-Manager (V1.17.1)
### ComfyUI Revision: 1861 on 'main' [ebc1e045] | Released on '2024-01-29'
FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json

Import times for custom nodes:
   0.0 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\canvas_tab
   0.0 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\step-slider
   0.0 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\comfyui-popup_preview
   0.0 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\efficiency-nodes-comfyui
   0.0 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\ComfyI2I
   0.0 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\comfyui-photoshop
   0.0 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\efficiency-nodes-comfyui-LACE
   0.2 seconds: C:\Users\silvr\OneDrive\Desktop\ComfyUI\custom_nodes\ComfyUI-Manager

Starting server

To see the GUI go to: http://127.0.0.1:8188
FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
got prompt
Requested to load SDXLClipModel
Loading 1 new model
C:\Users\silvr\OneDrive\Desktop\ComfyUI\comfy\ldm\modules\attention.py:318: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:281.)
  out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False)
----------------------------------------
[36mEfficient Loader (LACE) Models Cache:[0m
Ckpt:
  [1] sd_xl_turbo_1.0_fp16

>>>>>>>>>>>>> Mark 1 <<<<<<<<<<<<<
>>>>>>>>>> diffusion step from GUI = 0
Requested to load SDXL
Loading 1 new model
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 0
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 1
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 1
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 2
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 2
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 3
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 3
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 4
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 4
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


intermediate_latent type: <class 'list'>
intermediate_latent len: 5
intermediate_latent[0]: {'samples': tensor([[[[ -17.5462,  101.4616,  -90.7086,  ...,  -27.7100,   86.7369,
             27.9518],
          [ -10.8294,   92.0531,  -30.6974,  ...,  -18.0825,    4.0579,
             48.4662],
          [  -8.1208,   72.7919,  169.7122,  ...,  -15.3952,   44.7072,
           -163.0473],
          ...,
          [  86.6724,  -23.0780,  -52.6757,  ...,  -48.9347,  -12.0933,
             -8.5325],
          [  48.8975,  -87.0812,  -43.7057,  ...,   28.4045,   22.7184,
             92.8099],
          [  84.6138,   51.1260,  -24.9226,  ...,  -38.8815,  -64.3693,
             85.8055]],

         [[  98.5520,   34.9970,   46.5348,  ...,   36.2197, -137.3322,
             83.0570],
          [ 122.2613,  -53.1606, -112.6490,  ...,   55.3538,    5.1496,
            -29.4153],
          [  -9.7360,  -51.9681,  -29.4990,  ...,  -17.3904,   -7.8418,
            -23.4438],
          ...,
          [  -8.1315,  -63.7596,  -85.5796,  ...,    7.4093,   23.2165,
            -30.0840],
          [ -36.1749,   -9.9965,  -93.9598,  ...,   44.1305,   16.7799,
             -7.7446],
          [ -57.8534,  -21.2677,  -10.8351,  ...,   -8.9519,  -39.2365,
            -63.2387]],

         [[  41.0426,   -8.6971,  -37.1815,  ...,   76.1277,  -22.8122,
            -56.3132],
          [ -24.7765,   25.9252,   38.4220,  ...,   -6.9901,   59.4666,
             43.4510],
          [ -58.8247,  -36.2771,   -0.3270,  ...,  140.6446,   90.6690,
            136.3111],
          ...,
          [ 147.5197,   28.6044,   44.3038,  ...,  -19.1129,  -46.7050,
             47.3703],
          [ -53.5372,   28.6878,  -10.0484,  ...,  -56.1248, -131.1857,
            -35.3245],
          [  23.9261,   54.7911,   16.9250,  ...,   15.1516,  -81.8035,
             28.1350]],

         [[  87.9008,  -98.5493,  -61.1038,  ...,   71.0555,  -71.2501,
             58.2453],
          [ 108.0039,   13.6100,  -24.2537,  ...,  -37.0631,   53.1551,
             13.3496],
          [ -24.2440,   18.9247,  -17.9181,  ...,  -23.5122,   -2.9003,
            -14.6498],
          ...,
          [   6.3868,  -29.5704,  -57.5564,  ...,   45.5065,  -28.1076,
             43.9945],
          [  48.1183,    8.0956,  -39.2305,  ...,   19.7816,  102.1217,
           -159.3379],
          [   4.2791,   34.0890,   32.4036,  ...,   62.2032,   52.1689,
            -23.2615]]]])}
Requested to load AutoencoderKL
Loading 1 new model

>>>>>>>>>>>>> Mark 2 <<<<<<<<<<<<<
preview type: <class 'dict'>
preview keys: dict_keys(['images'])
preview images len: 1
preview['images]:[{'filename': 'ComfyUI_temp_givqu_00001_.png', 'subfolder': '', 'type': 'temp'}]

>>>>>>>>>>>>> Mark 3 <<<<<<<<<<<<<
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
>>>>>>>>>>>>> Mark 4 <<<<<<<<<<<<<
Prompt executed in 5.42 seconds
got prompt

>>>>>>>>>>>>> Mark 1 <<<<<<<<<<<<<
>>>>>>>>>> diffusion step from GUI = 0
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 0
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 1
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 1
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 2
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 2
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 3
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 3
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 4
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 4
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 5
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 5
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 6
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 6
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 7
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 7
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 8
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 8
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 9
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 9
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


intermediate_latent type: <class 'list'>
intermediate_latent len: 10
intermediate_latent[0]: {'samples': tensor([[[[ -40.8221,  -19.6986,  -56.9187,  ...,  -28.4179,  124.6993,
             24.8737],
          [ -88.9284,  -84.4381,  -10.8862,  ...,  -17.1074,  100.7184,
            -96.6743],
          [  -2.5633,  112.9327,  -26.3090,  ...,   41.6784,   31.3544,
            -10.9891],
          ...,
          [ -16.8440,  -87.1961,   95.1917,  ...,   18.3022,  -19.3673,
             29.5453],
          [ -69.7858,  -19.5133,   29.5545,  ...,  -16.9777,  -37.7245,
             25.8724],
          [  17.3198, -107.1516,  -23.3232,  ...,  -30.5119,  -25.1009,
            -40.7149]],

         [[  73.9492,  -36.4027,  -43.0773,  ...,   28.5780,    6.9575,
            -65.8402],
          [ -68.0067,   -7.7318,   88.6392,  ...,  123.3746,   36.0616,
             93.9643],
          [  41.9483,  -44.2562,   -4.2157,  ...,  -19.7217,   37.3744,
              6.6757],
          ...,
          [ -16.6878,  -42.6475,   -1.6708,  ...,   26.4175,  105.2239,
            -30.2873],
          [  48.1548, -106.2702,   56.7351,  ...,   75.6906,   16.5688,
           -118.6553],
          [ -88.8641,  -46.0014,  -39.6197,  ...,   -3.5841,   48.9478,
            -19.7535]],

         [[  93.1231,  -16.0814,  -37.5716,  ...,  -88.2771,    1.1059,
             57.1793],
          [ 112.8011,   67.7648,   66.8898,  ...,   65.5746,   34.1296,
             40.9176],
          [  25.8338,   46.3528,   39.0536,  ...,  -70.9325,    9.0859,
            -72.7824],
          ...,
          [  20.2893,  105.9076,   87.4812,  ...,  -96.2546,  -89.6845,
             36.2916],
          [  15.6773,   26.8488,  -70.7648,  ...,   13.7651,    7.1132,
             24.9907],
          [  -7.5512,   30.9635,   36.5035,  ...,   15.2456,   -5.5086,
            -14.8910]],

         [[  29.9223,  -54.1904,   52.6353,  ...,   22.5877,  -67.2540,
             34.1753],
          [  22.4385,   30.9112,   -0.6950,  ...,   71.4156,   79.0430,
            -61.6474],
          [ -21.7199, -159.4783,  -23.4658,  ...,    9.2761,   80.6923,
            -66.6927],
          ...,
          [ -77.8665, -124.6391,   68.2644,  ...,   33.7794,  -79.1597,
              1.1436],
          [ -13.9315,   62.5190,  -37.3004,  ...,   26.9024,  -63.8483,
             57.0332],
          [ -29.4769,  -85.2823,   42.5695,  ...,   78.3177,   17.7036,
             13.1513]]]])}

>>>>>>>>>>>>> Mark 2 <<<<<<<<<<<<<
preview type: <class 'dict'>
preview keys: dict_keys(['images'])
preview images len: 1
preview['images]:[{'filename': 'ComfyUI_temp_jpjpi_00001_.png', 'subfolder': '', 'type': 'temp'}]

>>>>>>>>>>>>> Mark 3 <<<<<<<<<<<<<
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
>>>>>>>>>>>>> Mark 4 <<<<<<<<<<<<<
Prompt executed in 4.77 seconds
got prompt
ERROR:root:Failed to validate prompt for output 18:
ERROR:root:* (prompt):
ERROR:root:  - Value 256 smaller than min of 320: tile_size
ERROR:root:* LACE Visualizer 18:
ERROR:root:  - Value 256 smaller than min of 320: tile_size
ERROR:root:Output will be ignored

>>>>>>>>>>>>> Mark 1 <<<<<<<<<<<<<
>>>>>>>>>> diffusion step from GUI = 0
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 0
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 1
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 1
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 2
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 2
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 3
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 3
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 4
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 4
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 5
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 5
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 6
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 6
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 7
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 7
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 8
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 8
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 9
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 9
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 10
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 10
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 11
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 11
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


intermediate_latent type: <class 'list'>
intermediate_latent len: 12
intermediate_latent[0]: {'samples': tensor([[[[ -37.5186,   18.0006,   51.5869,  ...,  -47.6701,   46.7619,
             28.0325],
          [  -8.5274,   28.5566,  113.4981,  ...,  -13.8153,   61.1641,
             49.8929],
          [  -9.7915,  119.2339, -124.9411,  ...,   67.4970,   -8.5536,
              0.4759],
          ...,
          [  84.5481,   36.7465,  -42.1736,  ...,  -23.1444,  -18.2096,
             34.7240],
          [  80.1887, -227.4194,   32.2885,  ...,   24.5429, -249.9008,
             50.6960],
          [ -81.3856,  -21.1354,   20.2986,  ...,   67.4887,   -4.6221,
             55.3744]],

         [[  99.1445,  -15.1435,   43.1652,  ...,   19.4102,  121.4475,
            -25.5703],
          [  83.5193,   92.2411,  -90.5484,  ...,   14.8682,  142.1388,
            -38.4663],
          [  46.9881,  -89.8361,  -11.9283,  ...,  -13.1524,  -21.9673,
             31.4747],
          ...,
          [  42.8771,  -65.3740,   38.1136,  ...,  -99.6170,  -27.2200,
            -21.5462],
          [  13.1482,  -54.9240,   50.3002,  ...,  -72.6088,   39.2759,
             44.8926],
          [ -98.4814,   57.8924,  154.5733,  ...,   44.1621,  -91.5846,
            111.7909]],

         [[ -36.4601,   14.5941,  -92.5409,  ...,  -40.8150, -106.1667,
            -59.1722],
          [ -11.5505,  -64.7681, -106.7755,  ...,  -33.2625,  -70.6847,
            -59.1910],
          [  17.4511,  -43.4561,   71.4934,  ...,  -20.5421,  -27.4960,
             -8.7325],
          ...,
          [ -17.1136,   25.1035,  -27.1721,  ...,  120.9249,  107.9900,
             59.6042],
          [  50.1689,  -41.6287,  -32.3430,  ...,  110.3350,   46.3822,
             -9.0878],
          [  33.5564,   99.5044,   16.6549,  ...,    5.4747,   61.4726,
             26.2452]],

         [[  10.1976,  -45.7266,  -52.8266,  ...,   66.0734,   18.4719,
            -46.7100],
          [-162.5130,  -20.9992,  -54.7915,  ...,  -78.7885,   -2.7045,
           -135.6793],
          [  21.4274,  -93.9360,   26.4611,  ...,   62.0962,    4.6129,
           -106.5516],
          ...,
          [  76.1121,  107.0352,  -49.3065,  ...,  -18.9658,  -51.2684,
            -50.3597],
          [  77.9890, -102.9762,   21.8756,  ...,  -33.8224,   61.0938,
            -50.5592],
          [   2.8688,   91.2059,  -72.9875,  ..., -149.2495,   22.0118,
            -19.7222]]]])}

>>>>>>>>>>>>> Mark 2 <<<<<<<<<<<<<
preview type: <class 'dict'>
preview keys: dict_keys(['images'])
preview images len: 1
preview['images]:[{'filename': 'ComfyUI_temp_dhdgz_00001_.png', 'subfolder': '', 'type': 'temp'}]

Prompt executed in 5.18 seconds
got prompt
ERROR:root:Failed to validate prompt for output 18:
ERROR:root:* (prompt):
ERROR:root:  - Value 256 smaller than min of 320: tile_size
ERROR:root:* LACE Visualizer 18:
ERROR:root:  - Value 256 smaller than min of 320: tile_size
ERROR:root:Output will be ignored

>>>>>>>>>>>>> Mark 1 <<<<<<<<<<<<<
>>>>>>>>>> diffusion step from GUI = 0
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 0
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 1
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 1
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 2
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 2
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 3
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 3
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 4
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 4
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 5
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 5
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 6
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 6
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 7
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 7
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 8
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 8
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 9
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 9
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 10
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 10
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 11
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 11
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


intermediate_latent type: <class 'list'>
intermediate_latent len: 12
intermediate_latent[0]: {'samples': tensor([[[[ 6.9452e+01,  4.2628e+01, -6.7247e+01,  ..., -1.7325e+01,
           -6.1307e+01,  7.1033e+01],
          [-1.9131e+01, -8.4161e-01, -3.0752e+01,  ..., -1.7671e+01,
            9.5071e+00, -5.0112e+01],
          [ 1.0867e+00,  3.0542e+01,  2.5332e+01,  ..., -1.1454e+02,
            7.1483e+01, -9.0278e+01],
          ...,
          [-5.1007e+01,  3.0452e+01, -9.9221e+01,  ...,  5.4229e+00,
           -6.6933e+01,  5.4091e+01],
          [ 4.4853e+01,  7.5211e+01,  5.5805e+01,  ..., -9.3426e+00,
            2.0491e+01,  1.7430e+01],
          [-9.4884e+01,  9.4402e+01,  1.1419e+02,  ...,  6.4271e+01,
            6.5263e+01, -9.3797e+01]],

         [[ 3.7330e+01, -4.9219e+01,  1.3190e+01,  ..., -1.2655e+02,
           -1.1509e+02,  8.8214e+01],
          [ 6.3300e+01, -2.1559e+01, -2.1655e+01,  ..., -3.2044e+01,
           -6.1328e+01,  1.0621e+01],
          [ 1.0198e+01, -1.3055e+01, -4.7541e+01,  ..., -1.2889e+01,
           -8.4032e-01,  1.1902e+02],
          ...,
          [-2.2468e+01, -3.8897e+00, -6.7138e-01,  ...,  7.4771e+00,
           -8.9437e+01, -3.4361e+01],
          [ 8.6588e+01,  6.6116e+01, -1.3012e+02,  ..., -1.2180e+02,
            3.1789e+01,  6.8004e+01],
          [-1.9626e+01,  3.0777e+01, -3.2712e+01,  ..., -1.0096e+02,
           -8.6207e+01, -9.8219e+01]],

         [[ 7.9066e+01, -6.4866e+01,  3.9772e+01,  ..., -1.5111e+02,
           -1.0129e+02,  5.7082e+01],
          [-5.2948e+01,  2.2143e+01,  1.5652e+02,  ...,  7.6010e+01,
            1.4641e+00, -1.0254e+01],
          [ 1.1628e+02, -4.8359e+01,  4.1185e+01,  ..., -5.0290e+01,
            1.2277e+01,  8.8766e+01],
          ...,
          [ 6.5254e+01, -1.9501e+01, -3.4178e+01,  ...,  8.6780e+01,
           -4.4496e+00,  2.8697e+01],
          [-8.8860e+00, -1.2723e+02, -7.0549e+01,  ...,  1.2200e-01,
            6.5830e+01,  7.2336e+01],
          [-2.9205e+01,  3.1568e+01, -9.5372e+00,  ...,  3.7934e+01,
           -6.1516e+01, -1.4452e+01]],

         [[ 1.2562e+02, -1.2568e+02, -5.9091e+01,  ...,  1.2022e+01,
           -5.7084e+01, -2.4024e+01],
          [-1.0847e+02, -4.7891e+01, -2.4693e+01,  ...,  3.1766e+01,
            1.1798e+01,  9.2301e+01],
          [ 2.7679e+01,  5.4631e+01,  6.9576e+01,  ...,  6.8703e+00,
            4.0602e+01, -1.0765e+01],
          ...,
          [-1.7298e+01,  2.1418e+00, -8.0083e+01,  ...,  2.0611e+01,
           -3.8062e+01, -1.3380e+02],
          [-1.0658e+02,  4.0711e+01, -9.4894e+01,  ...,  7.9666e+01,
           -7.4432e+01, -3.5813e+01],
          [-4.8017e+01,  7.4807e+01,  6.6180e+01,  ..., -4.1591e+01,
           -9.1171e+01, -5.1026e+01]]]])}

>>>>>>>>>>>>> Mark 2 <<<<<<<<<<<<<
preview type: <class 'dict'>
preview keys: dict_keys(['images'])
preview images len: 1
preview['images]:[{'filename': 'ComfyUI_temp_tcqdo_00001_.png', 'subfolder': '', 'type': 'temp'}]

Prompt executed in 5.07 seconds
got prompt

>>>>>>>>>>>>> Mark 1 <<<<<<<<<<<<<
>>>>>>>>>> diffusion step from GUI = 0
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 0
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 1
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 1
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 2
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 2
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 3
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 3
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 4
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 4
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 5
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 5
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 6
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 6
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 7
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 7
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 8
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 8
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 9
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 9
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 10
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 10
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 11
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 11
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


intermediate_latent type: <class 'list'>
intermediate_latent len: 12
intermediate_latent[0]: {'samples': tensor([[[[  36.6832, -166.5358,  -18.3520,  ...,   14.9233,  -98.9929,
             24.2101],
          [  39.5711,    0.7081,  -85.0072,  ...,   92.0616,  109.6659,
            -40.1321],
          [-101.0732,   42.2384,  -27.2126,  ...,  -21.6191,   10.3909,
            -48.6028],
          ...,
          [  97.4076,   19.8739,  -39.9285,  ...,   39.1101,   41.2885,
           -146.7912],
          [ -24.0048,   70.3839,   18.4101,  ...,   68.9061,   -5.6481,
            -44.3789],
          [ -77.6708,  -46.1321,   23.9875,  ...,   86.3549,  -71.2488,
             40.0851]],

         [[ -11.4326,   70.1561,  -55.8676,  ...,   -9.9356,   -2.2071,
            -11.7397],
          [   6.1981,    3.2390,   -0.8983,  ...,    3.2801,    1.0577,
            -26.0689],
          [  -0.6215,  -95.4994,   39.5855,  ...,  -33.1711,   48.7972,
            -46.0162],
          ...,
          [ -45.5566,   13.6244,   80.8524,  ...,   39.0724,   56.6438,
             -0.9499],
          [ -62.8058,    4.8164,    6.9207,  ...,   14.1541,  -18.2943,
             84.2397],
          [   2.2929,   12.9723,   44.2806,  ...,   12.6818,  107.2846,
             61.1312]],

         [[ -33.2188,  -13.0907,  103.1967,  ...,    1.5368,   96.4326,
             17.1794],
          [  98.4989,   62.8754,   96.7242,  ...,    1.6308,   94.2512,
            -74.5520],
          [ -78.8283,   52.3487,  123.0627,  ...,  -35.6898,  102.0471,
            -31.7695],
          ...,
          [ -91.2674,   59.5441,   33.5997,  ...,    4.6773,   -1.8079,
             52.2927],
          [ -78.1439,  -45.9811,  -76.0408,  ...,   66.4826,  -77.2671,
             95.5250],
          [  34.3013,  -23.7303,  -37.8006,  ...,   48.6389,   23.9439,
             38.6465]],

         [[  17.2964,    2.2466,  146.2644,  ...,  -82.5806, -131.5832,
             31.6486],
          [  16.6078,  -33.8036,   -9.3431,  ...,  -95.7588,   64.5419,
             -4.6042],
          [   2.1163,    8.5498, -110.1240,  ...,   12.8819,   23.4188,
            133.7387],
          ...,
          [   5.6661,    3.5368,  -33.4681,  ..., -119.1231,   78.1266,
            -25.7913],
          [ 102.1938,  -15.2390,   36.3265,  ...,   19.2332,   42.0441,
            -19.7854],
          [  76.2739,   38.1417,  -17.2497,  ...,  -13.7082,   90.8566,
              8.9180]]]])}

>>>>>>>>>>>>> Mark 2 <<<<<<<<<<<<<
preview type: <class 'dict'>
preview keys: dict_keys(['images'])
preview images len: 1
preview['images]:[{'filename': 'ComfyUI_temp_upnqs_00001_.png', 'subfolder': '', 'type': 'temp'}]

>>>>>>>>>>>>> Mark 3 <<<<<<<<<<<<<
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
>>>>>>>>>>>>> Mark 4 <<<<<<<<<<<<<
Prompt executed in 6.03 seconds
got prompt

>>>>>>>>>>>>> Mark 1 <<<<<<<<<<<<<
>>>>>>>>>> diffusion step from GUI = 0
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 0
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 1
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 1
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 2
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 2
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 3
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 3
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 4
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 4
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 5
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 5
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 6
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 6
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 7
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 7
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 8
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 8
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 9
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 9
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 10
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 10
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 11
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 11
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


intermediate_latent type: <class 'list'>
intermediate_latent len: 12
intermediate_latent[0]: {'samples': tensor([[[[ 4.2519e+01,  2.7919e+01,  1.1678e+01,  ...,  1.3030e+02,
           -2.7445e+01,  4.2389e+01],
          [-1.0271e+02,  7.8892e+01,  1.4362e+01,  ..., -2.0141e+01,
           -2.9125e+01,  9.5922e+01],
          [ 1.2100e+02,  1.3060e+01,  1.3711e+02,  ...,  1.0835e+02,
           -1.1353e+02, -1.2522e+02],
          ...,
          [-7.2507e+01,  1.1618e+02, -6.2508e+00,  ...,  1.2969e+02,
            1.3391e+02, -5.8845e+01],
          [ 4.6700e+01, -3.3634e+01,  3.7417e+01,  ..., -1.9050e+01,
           -9.3273e+01, -8.8886e+01],
          [ 9.3632e+01, -1.1693e+02,  2.6650e+01,  ...,  2.1354e-01,
            7.8471e+01, -1.0876e+02]],

         [[-9.8557e+01,  1.1652e+02,  2.4469e+01,  ..., -2.9028e+01,
            4.6759e+00, -6.4123e+01],
          [-9.3129e+01, -1.0897e+02, -9.6305e+01,  ..., -1.1291e+02,
           -3.0859e+01,  3.1396e+01],
          [ 6.4115e+01,  2.0873e+02, -2.7346e+01,  ...,  7.5823e+01,
           -4.0761e+01,  6.9907e+01],
          ...,
          [-9.7035e-01, -5.2144e+01,  2.1065e+01,  ...,  7.2999e+01,
            2.2349e+01, -1.8985e+02],
          [-1.2940e+02, -3.0141e+00, -8.8138e+01,  ..., -4.5583e+01,
            6.9719e+01,  1.3275e+01],
          [ 2.8347e+01,  2.2927e+01,  2.9231e+01,  ...,  8.6766e+01,
           -2.0186e+02,  6.5447e+01]],

         [[ 9.5889e+01,  5.4359e+01, -8.0884e+01,  ..., -7.0947e+01,
            7.8948e+01,  8.1401e+01],
          [-1.2791e+02, -9.7740e+00,  4.9088e+01,  ...,  1.2272e+01,
            3.3933e+01, -9.7765e-01],
          [ 3.8650e+01, -2.7365e+01, -1.4350e+02,  ..., -1.1848e+02,
           -8.4538e+01,  5.8502e+01],
          ...,
          [-5.0034e+01,  8.3937e+01,  1.9916e+01,  ...,  2.7007e+00,
           -1.5312e+01,  1.1331e+01],
          [ 1.7012e+01,  8.8554e+01, -1.8970e+01,  ...,  3.0127e+01,
           -1.2252e+02,  3.2471e+01],
          [-5.4719e+01, -1.8502e+02,  3.0265e+00,  ...,  1.8569e+01,
            2.1622e+01,  5.8430e+01]],

         [[ 4.9493e+01,  1.3966e+02,  1.6043e+01,  ...,  5.7029e+01,
           -7.2000e+01,  3.9545e+01],
          [ 1.0200e+01, -1.0429e+02,  3.1755e+01,  ..., -1.6263e+02,
            6.2171e+00, -9.4870e+01],
          [-4.9808e+01, -2.3702e+02,  7.6701e+00,  ...,  3.7857e+01,
           -7.6043e+01, -1.1528e+02],
          ...,
          [-9.3640e+01,  1.0811e+02, -6.0994e+01,  ..., -8.8791e+01,
           -8.3409e+01,  1.9123e+01],
          [-6.1857e+01,  2.8182e+01, -4.6836e+01,  ..., -8.4951e+01,
           -1.6839e+02,  4.2864e+01],
          [-7.3619e+01, -3.9581e+01, -5.5742e+01,  ...,  6.3256e+01,
           -2.9408e+00, -3.3688e+00]]]])}

>>>>>>>>>>>>> Mark 2 <<<<<<<<<<<<<
preview type: <class 'dict'>
preview keys: dict_keys(['images'])
preview images len: 1
preview['images]:[{'filename': 'ComfyUI_temp_ttnuv_00001_.png', 'subfolder': '', 'type': 'temp'}]

>>>>>>>>>>>>> Mark 3 <<<<<<<<<<<<<
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
>>>>>>>>>>>>> Mark 4 <<<<<<<<<<<<<
Prompt executed in 6.38 seconds
got prompt

>>>>>>>>>>>>> Mark 1 <<<<<<<<<<<<<
>>>>>>>>>> diffusion step from GUI = 0
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 0
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 1
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 1
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 2
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 2
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 3
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 3
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 4
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 4
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 5
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 5
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 6
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 6
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 7
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 7
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 8
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 8
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 9
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 9
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 10
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 10
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 11
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 11
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 12
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 12
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 13
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 13
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 14
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 14
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 15
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 15
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 16
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 16
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 17
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 17
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 18
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 18
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 19
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 19
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


intermediate_latent type: <class 'list'>
intermediate_latent len: 20
intermediate_latent[0]: {'samples': tensor([[[[ 130.3136,   49.5807,   98.5884,  ...,   42.9254,  -15.4570,
             92.8349],
          [  73.8209,   10.4774,   32.9101,  ...,   64.7275,  103.2721,
             68.2633],
          [  23.7246, -102.5903,  112.7540,  ...,  -56.4564, -227.9547,
             65.7072],
          ...,
          [-116.0439,    0.6641,  -40.2360,  ...,   88.5478,  -17.1632,
            146.4742],
          [ -58.8231,   14.6575,    5.4916,  ...,  -12.1688,  -79.6064,
           -131.5395],
          [-198.8195,   -1.8231,  -14.2964,  ...,  119.6870,   64.7513,
            -21.7210]],

         [[-115.3418, -128.3944,  -49.2733,  ...,   49.1198,  -81.3946,
             54.4596],
          [ 120.6824,   -9.4771,   -8.0588,  ...,   99.8160,   95.1830,
             37.1570],
          [-149.0379,   27.7892,  106.2166,  ...,   68.8086,  -96.6442,
             78.1361],
          ...,
          [  35.4709,    8.7712,   67.0294,  ...,  -42.1024,   43.6512,
            119.8278],
          [ 168.8015,    4.7956,   12.1537,  ..., -114.2980,   73.1509,
             -2.7012],
          [ -72.7667,  -17.6994,  -48.0776,  ...,   73.9234,   54.5741,
              4.3404]],

         [[ -78.8539,   34.1129,   78.4574,  ...,  -65.3475,   55.4605,
            -14.2363],
          [  46.7178,   16.5255,  -25.0850,  ..., -105.3884,  -27.4151,
             64.2238],
          [-114.1108,   15.4908,  -56.2134,  ..., -130.4219,    8.2720,
             13.6798],
          ...,
          [  -4.6175,   67.6668,   14.8952,  ...,   59.9090,   76.9286,
            -91.1088],
          [-132.0729,   30.0016,   98.4174,  ...,  -48.6280,  -13.5375,
            151.5379],
          [ -59.9946,   20.6073,  -29.1341,  ...,  -17.7541,  -86.8087,
            -60.2787]],

         [[-117.0715,   -8.8066,   48.4671,  ...,   -9.0649,   37.1730,
             27.3714],
          [  47.8327,  105.7656,   84.2770,  ...,   39.8598,  -84.0198,
            -21.5867],
          [ 150.8539,  -32.3616, -188.6730,  ..., -124.1509,   87.6287,
           -112.3846],
          ...,
          [  -9.8371,  -33.4515,  -59.2445,  ...,  -75.1672, -137.2593,
            -24.7474],
          [-212.1555,  102.8525,  132.4092,  ...,   23.8544,  -42.5030,
             50.6331],
          [  72.2853,  -38.1600,  -10.1636,  ..., -129.4064,  -21.7606,
            -17.4619]]]])}

>>>>>>>>>>>>> Mark 2 <<<<<<<<<<<<<
preview type: <class 'dict'>
preview keys: dict_keys(['images'])
preview images len: 1
preview['images]:[{'filename': 'ComfyUI_temp_lpciq_00001_.png', 'subfolder': '', 'type': 'temp'}]

>>>>>>>>>>>>> Mark 3 <<<<<<<<<<<<<
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
>>>>>>>>>>>>> Mark 4 <<<<<<<<<<<<<
Prompt executed in 15.58 seconds
got prompt

>>>>>>>>>>>>> Mark 1 <<<<<<<<<<<<<
>>>>>>>>>> diffusion step from GUI = 0
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 0
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 1
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 1
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 2
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 2
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 3
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 3
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 4
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 4
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 5
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 5
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 6
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 6
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 7
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 7
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 8
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 8
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


>>>>>>>>>> diffusion step from GUI = 9
sampler_name = euler_test
>>>>>>>>>> recieved diffusion_step = 9
samples type: <class 'dict'>
samples keys: dict_keys(['samples'])
samples shape: torch.Size([1, 4, 64, 64])


intermediate_latent type: <class 'list'>
intermediate_latent len: 10
intermediate_latent[0]: {'samples': tensor([[[[ -92.6629,  -13.9176,   19.5363,  ...,  -38.3916,   50.0129,
             12.5679],
          [  51.2163,  -46.9674,   -7.9320,  ...,  -47.7759,   58.7980,
            -68.6221],
          [   6.7474,   -3.1043,   15.7822,  ...,  -38.0052,   -3.1203,
            -66.6088],
          ...,
          [ 144.4651,  -47.6520,   -0.7869,  ...,   -9.0612,   78.0349,
             45.4324],
          [ -50.0328, -165.2097,  102.4426,  ...,  -64.2012,  -66.5724,
            -29.6286],
          [ -65.2404,   52.3818,  -16.0827,  ...,   87.5512,   34.7480,
             44.8455]],

         [[  45.5061,  -51.8297,  -44.4700,  ...,  -13.2440,  -38.7710,
           -107.2627],
          [ 124.7218,   16.1438,  -43.0721,  ...,   -8.3966,  -99.5438,
             45.8260],
          [ 100.2257,  114.3557,    0.9900,  ...,   16.8494,   67.3538,
            -25.3533],
          ...,
          [  28.8544,  -53.0141,  -63.0960,  ...,  -21.0962,   14.1999,
             -8.8964],
          [-111.7381,   -3.8937,  -48.4957,  ..., -181.7167,  -50.9818,
             27.8135],
          [  19.5883,  -29.0841,  -54.7981,  ...,  -26.1051,  -44.8600,
            -13.1850]],

         [[  59.1058,  -30.9056,   61.1648,  ...,  -60.0133,   -2.1228,
            -18.5684],
          [ -30.3453,  -58.2239,  -59.6351,  ...,    4.6297,  -64.5876,
             21.3248],
          [  -0.8349,   28.6257,   60.4032,  ..., -170.3444,   -6.3066,
            -14.9870],
          ...,
          [  -2.1984,   85.8579,    7.8190,  ...,  -87.6853,  -33.3806,
            -95.6578],
          [ -23.7906,   42.3239,  -41.1644,  ...,   22.7778, -209.3528,
            -63.1167],
          [ -97.0324,  -22.2465,  -80.0578,  ...,  117.8120,  113.0907,
            -22.1469]],

         [[   3.2132,  -28.4574,  169.3206,  ...,  -47.9263,   -3.9996,
           -164.3138],
          [  26.1932,   32.8259,  -65.8857,  ...,  -14.5464,   36.9731,
             -8.3082],
          [  20.0074,  -90.4232,  -63.9780,  ...,   18.7728,  -75.5507,
            -89.4977],
          ...,
          [  46.5507,  -81.0557,   81.7071,  ...,    9.7711,  112.1541,
            -70.1864],
          [  36.9702, -141.6409,  139.2879,  ...,   82.8342,   42.7354,
            -60.5694],
          [  14.6320,  -99.2278,  -34.6641,  ...,  -72.6007,  -55.3488,
            -61.1110]]]])}

>>>>>>>>>>>>> Mark 2 <<<<<<<<<<<<<
preview type: <class 'dict'>
preview keys: dict_keys(['images'])
preview images len: 1
preview['images]:[{'filename': 'ComfyUI_temp_kxcjk_00001_.png', 'subfolder': '', 'type': 'temp'}]

>>>>>>>>>>>>> Mark 3 <<<<<<<<<<<<<
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
image type: <class 'torch.Tensor'>
image shape: torch.Size([512, 512, 3])
i type: <class 'numpy.ndarray'>
i shape: (512, 512, 3)
Saved image size: (512, 512)
>>>>>>>>>>>>> Mark 4 <<<<<<<<<<<<<
Prompt executed in 4.80 seconds

Stopped server
